Goal
Create a CPU-based LLM inference service, containerized, runnable locally, with the same structure we would use in production.

Non-goals (for now)

No scaling

No batching

No streaming

No fine-tuning

This is a reference implementation, not a benchmark monster.
